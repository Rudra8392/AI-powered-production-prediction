# -*- coding: utf-8 -*-
"""Production GRU+ RMSprop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QVRbLfkyon5JoTkIA61NwZ3Ixcm-44sC
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from scipy import stats
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import os
import pickle  # For saving the scaler

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load dataset
df = pd.read_excel("/content/202 GL (1).xlsx", sheet_name="Sheet1")
df['Datetime'] = pd.to_datetime(df['Datetime'])
df.set_index('Datetime', inplace=True)

# Print initial shape for debugging
print(f"Initial DataFrame shape: {df.shape}")

# Handle missing values - first check where they are
print(f"Columns with missing values: {df.columns[df.isnull().any()].tolist()}")
print(f"Total missing values: {df.isnull().sum().sum()}")

# More robust handling of missing values
for col in df.columns:
    missing_count = df[col].isnull().sum()
    if missing_count > 0:
        if missing_count / len(df) > 0.5:  # If more than 50% missing, drop the column
            print(f"Dropping column {col} with {missing_count} missing values")
            df = df.drop(columns=[col])
        else:  # Otherwise fill with median
            print(f"Filling {missing_count} missing values in {col} with median")
            df[col] = df[col].fillna(df[col].median())

# Drop duplicate rows if any
df = df.drop_duplicates()
print(f"Shape after removing duplicates: {df.shape}")

# Select features and targets - check which columns actually exist first
all_cols = df.columns.tolist()
print(f"Available columns: {all_cols}")

# Define potential features and targets
potential_features = ['WHTP', 'DSP', 'Choke', 'THP', 'THT', 'Pressure DS Choke',
                     'Temperature DS Choke', 'CHP', 'OIL (BOPD)', 'GAS (MCF)', 'WATER (BWPD)']
targets = ['OIL (BOPD)', 'GAS (MCF)', 'WATER (BWPD)']

# Verify targets existence
for target in targets:
    if target not in all_cols:
        raise ValueError(f"Target column {target} not found in dataset")

# Filter features to only those available in the dataset
features = [f for f in potential_features if f in all_cols]
print(f"Using features: {features}")

# Create copy of original dataframe for fallback
df_original = df.copy()

# Check data types and convert numeric columns if needed
for col in features + targets:
    if not pd.api.types.is_numeric_dtype(df[col]):
        print(f"Converting {col} to numeric")
        df[col] = pd.to_numeric(df[col], errors='coerce')
        # Fill any new NaNs from conversion
        df[col] = df[col].fillna(df[col].median())

# Modified outlier handling with explicit bounds instead of Z-score
# This is more robust for financial/production data that might be naturally skewed
for col in features:
    q1 = df[col].quantile(0.01)  # More lenient lower bound (1st percentile)
    q3 = df[col].quantile(0.99)  # More lenient upper bound (99th percentile)
    iqr = q3 - q1
    lower_bound = q1 - (3 * iqr)
    upper_bound = q3 + (3 * iqr)

    # Count outliers before removing
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]
    if outliers > 0:
        print(f"Found {outliers} outliers in {col}")

    # Cap values instead of removing rows (more conservative)
    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

print(f"Shape after outlier handling: {df.shape}")

# Generate polynomial and interaction features (only if we have the necessary columns)
if 'Pressure DS Choke' in df.columns and 'Temperature DS Choke' in df.columns:
    df['Pressure_Temp_Interaction'] = df['Pressure DS Choke'] * df['Temperature DS Choke']
    print("Added interaction feature: Pressure_Temp_Interaction")

# Create rolling statistics only for key features with error handling
for col in ['WHTP', 'THP']:
    if col in df.columns:
        try:
            # Add window mean
            df[f'{col}_mean'] = df[col].rolling(window=12, min_periods=1).mean()
            # Add window std with handling for constant values
            std_values = df[col].rolling(window=12, min_periods=1).std()
            df[f'{col}_std'] = std_values.fillna(0)  # Fill NaN std with 0 (occurs when all values are identical)
            print(f"Added rolling statistics for {col}")
        except Exception as e:
            print(f"Error creating rolling statistics for {col}: {e}")

print("Added rolling statistics features")

# Generate time-based features
df['Hour'] = df.index.hour
df['DayOfWeek'] = df.index.dayofweek
df['DayOfMonth'] = df.index.day
print("Added time-based features")

# Create lag features for production metrics with smaller lag period
lag_period = min(12, len(df) // 10)  # Use at most 10% of data length for lag
print(f"Using lag period of {lag_period}")

for col in targets:
    if col in df.columns:
        df[f'{col}_lag'] = df[col].shift(lag_period)

print(f"Shape after creating lag features: {df.shape}")
print(f"Missing values after creating lag features: {df.isnull().sum().sum()}")

# Only drop initial rows where lag features are NaN (keeps more data)
df = df.iloc[lag_period:]
print(f"Shape after dropping initial {lag_period} rows: {df.shape}")

# Check for any remaining NaN values
if df.isnull().sum().sum() > 0:
    print("Warning: Still have missing values. Filling with feature medians.")
    for col in df.columns:
        if df[col].isnull().any():
            df[col] = df[col].fillna(df[col].median())

# Verify dataframe is not empty
if df.empty or len(df) < 2*lag_period:
    raise ValueError("DataFrame too small after preprocessing. Check your data.")

# Update and print final feature list
all_features = df.columns.tolist()
print(f"Final feature list before scaling: {all_features}")

# Separate features and targets for scaling
X_columns = [col for col in all_features if col not in targets]
y_columns = targets

# Scale features and targets separately with robust range handling
X_scaler = MinMaxScaler(feature_range=(0, 1))
y_scaler = MinMaxScaler(feature_range=(0, 1))

# Fit and transform X
X_values = df[X_columns].values
X_values_scaled = X_scaler.fit_transform(X_values)
df_scaled = pd.DataFrame(X_values_scaled, index=df.index, columns=X_columns)

# Fit and transform y
y_values = df[y_columns].values
y_values_scaled = y_scaler.fit_transform(y_values)
for i, col in enumerate(y_columns):
    df_scaled[col] = y_values_scaled[:, i]

# Split dataset into train (70%), validation (15%), and test (15%)
train_size = int(len(df_scaled) * 0.7)
val_size = int(len(df_scaled) * 0.15)

df_train = df_scaled.iloc[:train_size]
df_val = df_scaled.iloc[train_size:train_size+val_size]
df_test = df_scaled.iloc[train_size+val_size:]

print(f"Training set size: {len(df_train)}")
print(f"Validation set size: {len(df_val)}")
print(f"Test set size: {len(df_test)}")

# Function to create sequences
def create_sequences(data, seq_length, X_cols, y_cols):
    sequences_X, sequences_y = [], []

    for i in range(len(data) - seq_length):
        # Get sequence of X values
        seq_X = data.iloc[i:i+seq_length][X_cols].values
        # Get target y value (next timestep)
        seq_y = data.iloc[i+seq_length][y_cols].values

        sequences_X.append(seq_X)
        sequences_y.append(seq_y)

    return np.array(sequences_X), np.array(sequences_y)

# Use a smaller sequence length if dataset is small
seq_length = min(24, len(df_train) // 10)  # Use at most 10% of training data
print(f"Using sequence length of {seq_length}")

# Create sequences
X_train, y_train = create_sequences(df_train, seq_length, X_columns, y_columns)
X_val, y_val = create_sequences(df_val, seq_length, X_columns, y_columns)
X_test, y_test = create_sequences(df_test, seq_length, X_columns, y_columns)

print(f"Training sequences shape: {X_train.shape}, {y_train.shape}")
print(f"Validation sequences shape: {X_val.shape}, {y_val.shape}")
print(f"Test sequences shape: {X_test.shape}, {y_test.shape}")

# Check for NaN values in training data
if np.isnan(X_train).any() or np.isnan(y_train).any():
    raise ValueError("NaN values found in training data. Check preprocessing steps.")

# Define GRU model with proper input shape
model = Sequential()
model.add(GRU(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(0.2))
model.add(GRU(32))
model.add(Dropout(0.2))
model.add(Dense(16, activation='relu'))
model.add(Dense(len(targets)))

# Compile model with RMSprop optimizer and clipvalue to prevent exploding gradients
optimizer = tf.keras.optimizers.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    clipnorm=1.0
)
model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

# Create directory for model
model_dir = "./models"
if not os.path.exists(model_dir):
    os.makedirs(model_dir)

# Callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

checkpoint_path = os.path.join(model_dir, "best_gru_model.h5")
model_checkpoint = ModelCheckpoint(
    filepath=checkpoint_path,
    save_best_only=True,
    monitor='val_loss',
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

# Adaptive batch size based on dataset size
batch_size = min(32, len(X_train) // 10)
batch_size = max(8, batch_size)  # Ensure batch size is at least 8
print(f"Using batch size of {batch_size}")

# Train model with safety checks
try:
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=50,
        batch_size=batch_size,
        callbacks=[early_stopping, model_checkpoint, reduce_lr],
        verbose=1
    )

    # Plot training history
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('GRU Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')
    plt.savefig(os.path.join(model_dir, 'gru_training_history.png'))

    # Load best model weights
    if os.path.exists(checkpoint_path):
        model.load_weights(checkpoint_path)
        print(f"Loaded best model weights from {checkpoint_path}")

    # Predict on test set
    y_pred = model.predict(X_test)

    # Inverse transform predictions and actual values for evaluation
    y_test_inv = y_scaler.inverse_transform(y_test)
    y_pred_inv = y_scaler.inverse_transform(y_pred)

    # Calculate metrics
    for i, target in enumerate(targets):
        mae = np.mean(np.abs(y_test_inv[:, i] - y_pred_inv[:, i]))
        rmse = np.sqrt(np.mean((y_test_inv[:, i] - y_pred_inv[:, i])**2))
        mape = np.mean(np.abs((y_test_inv[:, i] - y_pred_inv[:, i]) / (y_test_inv[:, i] + 1e-5))) * 100
        print(f"{target} MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%")

        # Plot predictions vs actual for each target
        plt.figure(figsize=(12, 6))
        plt.plot(y_test_inv[:, i], label='Actual')
        plt.plot(y_pred_inv[:, i], label='Predicted')
        plt.title(f'{target} - Actual vs Predicted (GRU Model)')
        plt.legend()
        plt.savefig(os.path.join(model_dir, f'{target}_gru_predictions.png'))

    # Save model and scalers
    final_model_path = os.path.join(model_dir, "gru_production_forecasting.h5")
    model.save(final_model_path)

    # Save scalers
    with open(os.path.join(model_dir, "x_scaler_gru.pkl"), 'wb') as f:
        pickle.dump(X_scaler, f)

    with open(os.path.join(model_dir, "y_scaler_gru.pkl"), 'wb') as f:
        pickle.dump(y_scaler, f)

    print(f"Model saved to: {final_model_path}")
    print(f"Scalers saved to the models directory")
    print("GRU model training and evaluation completed successfully!")

    # Compare with previous model if exists
    lstm_model_path = os.path.join(model_dir, "lstm_production_forecasting.h5")
    if os.path.exists(lstm_model_path):
        print("\nComparison between GRU and LSTM models:")
        print("----------------------------------------")
        print("GRU advantages:")
        print("- Less complex architecture (fewer parameters)")
        print("- Often faster training times")
        print("- May perform better with smaller datasets")

        print("\nYou can further compare performance metrics between models")
        print("by running both and comparing the MAE, RMSE, and MAPE values")

except Exception as e:
    print(f"Error during GRU model training or evaluation: {e}")
    import traceback
    traceback.print_exc()

# Function to create a visualization comparing feature importance
def analyze_feature_importance():
    try:
        # Create a simplified model for feature analysis
        feature_model = Sequential()
        feature_model.add(GRU(32, input_shape=(seq_length, X_train.shape[2])))
        feature_model.add(Dense(len(targets)))
        feature_model.compile(optimizer='rmsprop', loss='mse')

        # Train briefly just to get some weights
        feature_model.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=0)

        # Get feature weights from first layer
        weights = np.abs(feature_model.layers[0].get_weights()[0])
        feature_importance = np.sum(weights, axis=(0, 1))

        # Normalize to get relative importance
        feature_importance = feature_importance / np.max(feature_importance)

        # Create bar chart of feature importance
        plt.figure(figsize=(14, 8))
        plt.bar(X_columns, feature_importance)
        plt.title('Relative Feature Importance for Production Prediction')
        plt.xlabel('Features')
        plt.ylabel('Relative Importance')
        plt.xticks(rotation=90)
        plt.tight_layout()
        plt.savefig(os.path.join(model_dir, 'feature_importance.png'))
        print("Feature importance analysis completed and saved")

    except Exception as e:
        print(f"Could not analyze feature importance: {e}")

# Run feature importance analysis
analyze_feature_importance()